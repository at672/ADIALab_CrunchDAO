{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "          model_directory_path: str = \"resources\") -> None:\n",
    "    \"\"\"\n",
    "    Do your model training here.\n",
    "    At each retrain this function will have to save an updated version of\n",
    "    the model under the model_directiory_path, as in the example below.\n",
    "    Note: You can use other serialization methods than joblib.dump(), as\n",
    "    long as it matches what reads the model in infer().\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: the data to train the model.\n",
    "        model_directory_path: the path to save your updated model\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #flush the memory\n",
    "    ## For model construction\n",
    "    #     alpha = 5\n",
    "    # Num_hidden_limit = X_train.shape[0] / ( alpha * (X_train.shape[1] + 1) )\n",
    "    # #alpha = 2 --> 800\n",
    "    # #alpha = 5 --> 320\n",
    "    # #alpha = 10 --> 160\n",
    "\n",
    "    train_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1] - 2\n",
    "    #can this fit in memory?\n",
    "    batch_size = 74267\n",
    "    #needs to be small enough that it can fit in memory. 2500 x 461 should be good\n",
    "    batch_size = 2500\n",
    "\n",
    "    ## do not include batch size when constructing input layer.\n",
    "    model = tf.keras.models.Sequential([\n",
    "        #tf.keras.layers.Flatten(input_shape = X_train.shape),\n",
    "        tf.keras.layers.Input(shape = (None, train_samples, n_features), batch_size = batch_size, name = \"Input_Layer\"),\n",
    "        tf.keras.layers.Dense(400, activation='swish'),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(300, activation='swish'),\n",
    "        tf.keras.layers.Dropout(0.02),\n",
    "        tf.keras.layers.Dense(200, activation='swish', activity_regularizer = tf.keras.regularizers.L2(0.01)),\n",
    "        tf.keras.layers.Dropout(0.02),\n",
    "        tf.keras.layers.Dense(100, activation='swish', activity_regularizer = tf.keras.regularizers.L2(0.01)),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(10, activation ='relu'),\n",
    "        tf.keras.layers.Dense(1, activation = None)\n",
    "    ])\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    my_callbacks = [ callback ]\n",
    "    #if using KL Divergence as loss, don't add it as a metric\n",
    "    model.compile(\n",
    "        optimizer= tf.keras.optimizers.Adam(learning_rate=1e-3) ,\n",
    "        #optimizer = tf.keras.optimizers.experimental.SGD( learning_rate = 0.001),\n",
    "        #loss = tf.keras.losses.BinaryCrossentropy(from_logits = True) ,\n",
    "        loss = tf.keras.losses.KLDivergence(),\n",
    "        )\n",
    "\n",
    "    # #Can do this if wanted\n",
    "    # model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=1e-3) ,\n",
    "    #     #loss = tf.keras.losses.BinaryCrossentropy(from_logits = True) ,\n",
    "    #     loss = 'mse',\n",
    "    #     metrics = [tf.keras.metrics.KLDivergence()]\n",
    "    #     )\n",
    "\n",
    "    # training the model\n",
    "    print(\"training...\")\n",
    "    history = model.fit(\n",
    "        x = X_train.iloc[:,2:],\n",
    "        y = y_train.iloc[:,2:],\n",
    "        epochs = 15,\n",
    "        steps_per_epoch = int( np.ceil(train_samples / batch_size)),\n",
    "        validation_split = 0.1,\n",
    "        shuffle = False,\n",
    "        workers = 8,\n",
    "        use_multiprocessing = True,\n",
    "        verbose = 1\n",
    "        callbacks = my_callbacks\n",
    "     )\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "    ## SAVE THE MODEL\n",
    "    # make sure that the train function correctly save the trained model\n",
    "    # in the model_directory_path\n",
    "    model_pathname = Path(model_directory_path) / \"model.joblib\"\n",
    "    print(f\"Saving model in {model_pathname}\")\n",
    "    joblib.dump(model, model_pathname)\n",
    "\n",
    "\n",
    "def infer(X_test: pd.DataFrame,\n",
    "          model_directory_path: str = \"resources\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Do your inference here.\n",
    "    This function will load the model saved at the previous iteration and use\n",
    "    it to produce your inference on the current date.\n",
    "    It is mandatory to send your inferences with the ids so the system\n",
    "    can match it correctly.\n",
    "    \n",
    "    Args:\n",
    "        model_directory_path: the path to the directory to the directory in wich we will be saving your updated model.\n",
    "        X_test: the independant  variables of the current date passed to your model.\n",
    "\n",
    "    Returns:\n",
    "        A dataframe (date, id, value) with the inferences of your model for the current date.\n",
    "    \"\"\"\n",
    "\n",
    "    # loading the model saved by the train function at previous iteration\n",
    "    model = joblib.load(Path(model_directory_path) / \"model.joblib\")\n",
    "    \n",
    "    # creating the predicted label dataframe with correct dates and ids\n",
    "    y_test_predicted = X_test[[\"date\", \"id\"]].copy()\n",
    "    y_test_predicted[\"value\"] = model.predict(X_test.iloc[:, 2:])\n",
    "\n",
    "    return y_test_predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crunch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
