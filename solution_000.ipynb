{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADIA LAB CrunchDAO Notebook Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up workspace cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 0\n",
    "!pip3 install crunch-cli --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "## STEP 2: Run this cell to load the data\n",
    "import crunch\n",
    "crunch = crunch.load_notebook(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stale-avi: already exists (use --force to override)\n",
      "c:\\GitHub\\CrunchDAO\\stale-avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "## STEP 3\n",
    "!crunch --notebook setup stale-avi --token 4tT3eWdXumNoCG7b7FKZJLN17fYnww6gSl6yeHy4PdsYPeqTnrKiq49hRb4sdmm2\n",
    "%cd stale-avi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPORTS I MAY USE\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # This is not advised in general, but it is used in this notebook to clean the presentation of results\n",
    "\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from hyperopt import hp, fmin, tpe, Trials\n",
    "#from hyperopt.pyll.base import scope\n",
    "#from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import dump, load\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "#import datatable as dtable\n",
    "import typing\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "          model_directory_path: str = \"resources\") -> None:\n",
    "    \"\"\"\n",
    "    Do your model training here.\n",
    "    At each retrain this function will have to save an updated version of\n",
    "    the model under the model_directiory_path, as in the example below.\n",
    "    Note: You can use other serialization methods than joblib.dump(), as\n",
    "    long as it matches what reads the model in infer().\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: the data to train the model.\n",
    "        model_directory_path: the path to save your updated model\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #flush the memory\n",
    "    ## For model construction\n",
    "    #     alpha = 5\n",
    "    # Num_hidden_limit = X_train.shape[0] / ( alpha * (X_train.shape[1] + 1) )\n",
    "    # #alpha = 2 --> 800\n",
    "    # #alpha = 5 --> 320\n",
    "    # #alpha = 10 --> 160\n",
    "\n",
    "    tf.random.set_seed(6741)\n",
    "\n",
    "    train_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1] - 2\n",
    "    #can this fit in memory?\n",
    "    batch_size = 74267\n",
    "    #needs to be small enough that it can fit in memory. 2500 x 461 should be good\n",
    "    batch_size = 2500\n",
    "    #batch_size = 1000\n",
    "\n",
    "    ## do not include batch size when constructing input layer.\n",
    "    model = tf.keras.models.Sequential([\n",
    "        #tf.keras.layers.Flatten(input_shape = X_train.shape),\n",
    "        tf.keras.layers.Input(shape = (None, train_samples, n_features), batch_size = batch_size, name = \"Input_Layer\"),\n",
    "        tf.keras.layers.Dense(800, activation='swish', activity_regularizer = tf.keras.regularizers.L2(0.03)),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(600, activation='tanh', activity_regularizer = tf.keras.regularizers.L2(0.03)),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(400, activation='swish', activity_regularizer = tf.keras.regularizers.L2(0.03)),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(200, activation='tanh', activity_regularizer = tf.keras.regularizers.L2(0.03)),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(10, activation ='tanh'),\n",
    "        tf.keras.layers.Dense(1, activation = None)\n",
    "    ])\n",
    "\n",
    "    #monitor = val_loss\n",
    "    #es = EarlyStopping(monitor = 'val_action_AUC', min_delta = 1e-4, patience = 10, mode = 'max', \n",
    "    #                       baseline = None, restore_best_weights = True, verbose = 0)\n",
    "\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience = 2, restore_best_weights = True)\n",
    "    \n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=2, min_lr=0.001)\n",
    "    #my_callbacks = [ reduce_lr, early_stopping ]\n",
    "    my_callbacks = [ early_stopping ]\n",
    "    #if using KL Divergence as loss, don't add it as a metric\n",
    "    model.compile(\n",
    "        optimizer= tf.keras.optimizers.Adam(learning_rate = 0.001) ,\n",
    "        #optimizer = tf.keras.optimizers.experimental.SGD( learning_rate = 0.001),\n",
    "        #loss = tf.keras.losses.BinaryCrossentropy(from_logits = True) ,\n",
    "        #loss = tf.keras.losses.KLDivergence(),\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "        )\n",
    "\n",
    "    # #Can do this if wanted\n",
    "    # model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=1e-3) ,\n",
    "    #     #loss = tf.keras.losses.BinaryCrossentropy(from_logits = True) ,\n",
    "    #     loss = 'mse',\n",
    "    #     metrics = [tf.keras.metrics.KLDivergence()]\n",
    "    #     )\n",
    "\n",
    "    # training the model\n",
    "    print(\"training...\")\n",
    "    history = model.fit(\n",
    "        x = X_train.iloc[:,2:],\n",
    "        y = y_train.iloc[:,2:],\n",
    "        epochs = 10,\n",
    "        steps_per_epoch = int( np.ceil(train_samples / batch_size)),\n",
    "        validation_split = 0.3,\n",
    "        shuffle = True,\n",
    "        workers = 8,\n",
    "        use_multiprocessing = True,\n",
    "        verbose = 1,\n",
    "        callbacks = my_callbacks\n",
    "     )\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "    ## SAVE THE MODEL\n",
    "    # make sure that the train function correctly save the trained model\n",
    "    # in the model_directory_path\n",
    "    model_pathname = Path(model_directory_path) / \"model.joblib\"\n",
    "    print(f\"Saving model in {model_pathname}\")\n",
    "    joblib.dump(model, model_pathname)\n",
    "\n",
    "    #Save history\n",
    "    # convert the history.history dict to a pandas DataFrame:     \n",
    "    # TODO: Figure out how to catch a PermissionError so this doesn't fail on the cloud.\n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    hist_csv_file = 'resources/epoch_history.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "\n",
    "def infer(X_test: pd.DataFrame,\n",
    "          model_directory_path: str = \"resources\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Do your inference here.\n",
    "    This function will load the model saved at the previous iteration and use\n",
    "    it to produce your inference on the current date.\n",
    "    It is mandatory to send your inferences with the ids so the system\n",
    "    can match it correctly.\n",
    "    \n",
    "    Args:\n",
    "        model_directory_path: the path to the directory to the directory in wich we will be saving your updated model.\n",
    "        X_test: the independant  variables of the current date passed to your model.\n",
    "\n",
    "    Returns:\n",
    "        A dataframe (date, id, value) with the inferences of your model for the current date.\n",
    "    \"\"\"\n",
    "\n",
    "    # loading the model saved by the train function at previous iteration\n",
    "    model = joblib.load(Path(model_directory_path) / \"model.joblib\")\n",
    "    \n",
    "    # creating the predicted label dataframe with correct dates and ids\n",
    "    y_test_predicted = X_test[[\"date\", \"id\"]].copy()\n",
    "    y_test_predicted[\"value\"] = model.predict(X_test.iloc[:, 2:])\n",
    "\n",
    "    return y_test_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Locally - Part 1: Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\\X_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_train.parquet\n",
      "already exists: file length match\n",
      "download data\\y_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/y_train.parquet\n",
      "already exists: file length match\n",
      "download data\\X_test.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_test_reduced.parquet\n",
      "already exists: file length match\n",
      "download data\\y_test.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/y_test_reduced.parquet\n",
      "already exists: file length match\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# Getting the data\n",
    "X_train, y_train, X_test = crunch.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting (X_train, y_train) in X_train_local, X_test_local, y_train_local, y_test_local\n",
      "X_train_local shape  (542200, 463)\n",
      "X_test_local shape  (200470, 463)\n",
      "y_train_local shape  (542200, 3)\n",
      "y_test_local shape  (200470, 3)\n"
     ]
    }
   ],
   "source": [
    "def temporal_train_test_split(X_train_loc, y_train_loc, test_size=0.2):\n",
    "    unique_dates = X_train_loc.date.unique()\n",
    "    split_date = unique_dates[int(len(unique_dates)*(1-test_size))]\n",
    "    X_train_local = X_train_loc[X_train_loc['date'] <= split_date]\n",
    "    X_test_local = X_train_loc[X_train_loc['date'] > split_date]\n",
    "    \n",
    "    y_train_local = y_train_loc[y_train_loc['date'] <= split_date]\n",
    "    y_test_local = y_train_loc[y_train_loc['date'] > split_date]\n",
    "    \n",
    "    return X_train_local, X_test_local, y_train_local, y_test_local\n",
    "\n",
    "print(\"Splitting (X_train, y_train) in X_train_local, X_test_local, y_train_local, y_test_local\")\n",
    "X_train_local, X_test_local, y_train_local, y_test_local = temporal_train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.20\n",
    ")\n",
    "\n",
    "print(\"X_train_local shape \", X_train_local.shape)\n",
    "print(\"X_test_local shape \", X_test_local.shape)\n",
    "print(\"y_train_local shape \", y_train_local.shape)\n",
    "print(\"y_test_local shape \", y_test_local.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Epoch 1/10\n",
      "217/217 [==============================] - 41s 183ms/step - loss: 1.3136 - val_loss: 0.4554\n",
      "Epoch 2/10\n",
      "217/217 [==============================] - 38s 177ms/step - loss: 0.3933 - val_loss: 0.3667\n",
      "Epoch 3/10\n",
      "217/217 [==============================] - 40s 183ms/step - loss: 0.3524 - val_loss: 0.3490\n",
      "Epoch 4/10\n",
      "217/217 [==============================] - 36s 167ms/step - loss: 0.3427 - val_loss: 0.3426\n",
      "Epoch 5/10\n",
      "217/217 [==============================] - 36s 166ms/step - loss: 0.3395 - val_loss: 0.3403\n",
      "Epoch 6/10\n",
      "217/217 [==============================] - 36s 166ms/step - loss: 0.3381 - val_loss: 0.3388\n",
      "Epoch 7/10\n",
      "217/217 [==============================] - 36s 165ms/step - loss: 0.3375 - val_loss: 0.3390\n",
      "Epoch 8/10\n",
      "217/217 [==============================] - 36s 165ms/step - loss: 0.3372 - val_loss: 0.3378\n",
      "Epoch 9/10\n",
      "217/217 [==============================] - 36s 166ms/step - loss: 0.3368 - val_loss: 0.3386\n",
      "Epoch 10/10\n",
      "217/217 [==============================] - 36s 165ms/step - loss: 0.3367 - val_loss: 0.3389\n",
      "Training complete.\n",
      "Saving model in resources\\model.joblib\n",
      "#############\n",
      "Inference\n",
      "6265/6265 [==============================] - 24s 4ms/step\n",
      "Spearman's correlation 2.9919445177624753\n"
     ]
    }
   ],
   "source": [
    "#flush memory\n",
    "tf.keras.backend.clear_session()\n",
    "# Training. It may require a few minutes.\n",
    "train(X_train_local, y_train_local)\n",
    "\n",
    "print(\"#############\")\n",
    "print(\"Inference\")\n",
    "y_test_local_pred = infer(X_test_local, model_directory_path=\"resources\")\n",
    "score = spearmanr(y_test_local[\"y\"], y_test_local_pred[\"value\"])[0] * 100\n",
    "print(f\"Spearman's correlation {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model Locally Part 2: Actual Test Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove unused data to release memory\n"
     ]
    }
   ],
   "source": [
    "print(\"Remove unused data to release memory\")\n",
    "del X_train, y_train, X_test, X_train_local, X_test_local, y_train_local, y_test_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignoring cell #19: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 55)\n",
      "ignoring cell #23: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 55)\n",
      "ignoring cell #45: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m09:48:18\u001b[0m \u001b[31mforbidden library: requests\u001b[0m\n",
      "\u001b[32m09:48:18\u001b[0m \u001b[33m\u001b[0m\n",
      "\u001b[32m09:48:18\u001b[0m running local test\n",
      "\u001b[32m09:48:18\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
      "\u001b[32m09:48:18\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\\X_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_train.parquet\n",
      "already exists: file length match\n",
      "download data\\y_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/y_train.parquet\n",
      "already exists: file length match\n",
      "download data\\X_test.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_test_reduced.parquet\n",
      "already exists: file length match\n",
      "download data\\y_test.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/y_test_reduced.parquet\n",
      "already exists: file length match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m09:48:21\u001b[0m \u001b[33m---\u001b[0m\n",
      "\u001b[32m09:48:21\u001b[0m \u001b[33mloop: moon=269 train=True (1/5)\u001b[0m\n",
      "\u001b[32m09:48:21\u001b[0m \u001b[33mcall: train\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Epoch 1/20\n",
      "296/296 [==============================] - 25s 81ms/step - loss: 0.9868 - val_loss: 0.3759\n",
      "Epoch 2/20\n",
      "296/296 [==============================] - 25s 84ms/step - loss: 0.3579 - val_loss: 0.3431\n",
      "Epoch 3/20\n",
      "296/296 [==============================] - 25s 84ms/step - loss: 0.3415 - val_loss: 0.3382\n",
      "Epoch 4/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3383 - val_loss: 0.3367\n",
      "Epoch 5/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3374 - val_loss: 0.3365\n",
      "Epoch 6/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3371 - val_loss: 0.3366\n",
      "Epoch 7/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3372 - val_loss: 0.3361\n",
      "Epoch 8/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3372 - val_loss: 0.3367\n",
      "Epoch 9/20\n",
      "296/296 [==============================] - 25s 83ms/step - loss: 0.3369 - val_loss: 0.3370\n",
      "Epoch 10/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3369 - val_loss: 0.3368\n",
      "Epoch 11/20\n",
      "296/296 [==============================] - 24s 82ms/step - loss: 0.3364 - val_loss: 0.3368\n",
      "Epoch 12/20\n",
      "296/296 [==============================] - 24s 82ms/step - loss: 0.3365 - val_loss: 0.3361\n",
      "Epoch 13/20\n",
      "296/296 [==============================] - 24s 81ms/step - loss: 0.3361 - val_loss: 0.3359\n",
      "Epoch 14/20\n",
      "296/296 [==============================] - 23s 79ms/step - loss: 0.3360 - val_loss: 0.3366\n",
      "Epoch 15/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3361 - val_loss: 0.3366\n",
      "Epoch 16/20\n",
      "296/296 [==============================] - 23s 79ms/step - loss: 0.3359 - val_loss: 0.3369\n",
      "Epoch 17/20\n",
      "296/296 [==============================] - 24s 81ms/step - loss: 0.3359 - val_loss: 0.3366\n",
      "Epoch 18/20\n",
      "296/296 [==============================] - 24s 81ms/step - loss: 0.3358 - val_loss: 0.3367\n",
      "Epoch 19/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3358 - val_loss: 0.3363\n",
      "Epoch 20/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3358 - val_loss: 0.3360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m09:56:22\u001b[0m \u001b[33mcall: infer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Saving model in resources\\model.joblib\n",
      "125/125 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m09:56:23\u001b[0m \u001b[33m---\u001b[0m\n",
      "\u001b[32m09:56:23\u001b[0m \u001b[33mloop: moon=270 train=True (2/5)\u001b[0m\n",
      "\u001b[32m09:56:23\u001b[0m \u001b[33mcall: train\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Epoch 1/20\n",
      "298/298 [==============================] - 28s 85ms/step - loss: 0.9873 - val_loss: 0.3756\n",
      "Epoch 2/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3585 - val_loss: 0.3442\n",
      "Epoch 3/20\n",
      "298/298 [==============================] - 25s 85ms/step - loss: 0.3423 - val_loss: 0.3391\n",
      "Epoch 4/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3387 - val_loss: 0.3372\n",
      "Epoch 5/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3375 - val_loss: 0.3367\n",
      "Epoch 6/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3371 - val_loss: 0.3366\n",
      "Epoch 7/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3372 - val_loss: 0.3365\n",
      "Epoch 8/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3374 - val_loss: 0.3369\n",
      "Epoch 9/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3372 - val_loss: 0.3370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:00:17\u001b[0m \u001b[33mcall: infer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Saving model in resources\\model.joblib\n",
      "131/131 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:00:18\u001b[0m \u001b[33m---\u001b[0m\n",
      "\u001b[32m10:00:18\u001b[0m \u001b[33mloop: moon=271 train=False (3/5)\u001b[0m\n",
      "\u001b[32m10:00:18\u001b[0m \u001b[33mcall: infer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:00:18\u001b[0m \u001b[33m---\u001b[0m\n",
      "\u001b[32m10:00:18\u001b[0m \u001b[33mloop: moon=272 train=True (4/5)\u001b[0m\n",
      "\u001b[32m10:00:18\u001b[0m \u001b[33mcall: train\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Epoch 1/20\n",
      "301/301 [==============================] - 26s 83ms/step - loss: 0.9865 - val_loss: 0.3753\n",
      "Epoch 2/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3582 - val_loss: 0.3433\n",
      "Epoch 3/20\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.3419 - val_loss: 0.3385\n",
      "Epoch 4/20\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.3387 - val_loss: 0.3372\n",
      "Epoch 5/20\n",
      "301/301 [==============================] - 25s 84ms/step - loss: 0.3379 - val_loss: 0.3366\n",
      "Epoch 6/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3378 - val_loss: 0.3366\n",
      "Epoch 7/20\n",
      "301/301 [==============================] - 25s 84ms/step - loss: 0.3376 - val_loss: 0.3365\n",
      "Epoch 8/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3372 - val_loss: 0.3375\n",
      "Epoch 9/20\n",
      "301/301 [==============================] - 26s 85ms/step - loss: 0.3369 - val_loss: 0.3361\n",
      "Epoch 10/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3367 - val_loss: 0.3359\n",
      "Epoch 11/20\n",
      "301/301 [==============================] - 25s 85ms/step - loss: 0.3362 - val_loss: 0.3356\n",
      "Epoch 12/20\n",
      "301/301 [==============================] - 25s 84ms/step - loss: 0.3360 - val_loss: 0.3361\n",
      "Epoch 13/20\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.3359 - val_loss: 0.3359\n",
      "Epoch 14/20\n",
      "301/301 [==============================] - 24s 81ms/step - loss: 0.3358 - val_loss: 0.3370\n",
      "Epoch 15/20\n",
      "301/301 [==============================] - 25s 84ms/step - loss: 0.3362 - val_loss: 0.3353\n",
      "Epoch 16/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3356 - val_loss: 0.3360\n",
      "Epoch 17/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3355 - val_loss: 0.3360\n",
      "Epoch 18/20\n",
      "301/301 [==============================] - 24s 81ms/step - loss: 0.3358 - val_loss: 0.3358\n",
      "Epoch 19/20\n",
      "301/301 [==============================] - 26s 85ms/step - loss: 0.3354 - val_loss: 0.3359\n",
      "Epoch 20/20\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.3355 - val_loss: 0.3358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:08:41\u001b[0m \u001b[33mcall: infer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Saving model in resources\\model.joblib\n",
      "128/128 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:08:42\u001b[0m \u001b[33m---\u001b[0m\n",
      "\u001b[32m10:08:42\u001b[0m \u001b[33mloop: moon=273 train=False (5/5)\u001b[0m\n",
      "\u001b[32m10:08:42\u001b[0m \u001b[33mcall: infer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:08:43\u001b[0m \u001b[33mprediction_path=data\\prediction.csv\u001b[0m\n",
      "\u001b[32m10:08:43\u001b[0m \u001b[33mduration: time=00:20:24\u001b[0m\n",
      "\u001b[32m10:08:43\u001b[0m \u001b[33mmemory: before=\"5.77 GB\" after=\"8.70 GB\" consumed=\"2.93 GB\"\u001b[0m\n",
      "\u001b[32m10:08:43\u001b[0m \u001b[33mlocal test succesfully run!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>269</td>\n",
       "      <td>c6e83eda40042dab1af117e195d542f00a417627e3173a...</td>\n",
       "      <td>-0.036208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>269</td>\n",
       "      <td>97ae3194605438cbd2c59a3827f7c615dafa40d6cc3f42...</td>\n",
       "      <td>0.015091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>269</td>\n",
       "      <td>310382927ec56f64c6f2f834fd320c9f732e26df639e67...</td>\n",
       "      <td>-0.004857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>269</td>\n",
       "      <td>6e963f295f4ec1dc921be47638dba304f486ab2efd313c...</td>\n",
       "      <td>-0.031028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>269</td>\n",
       "      <td>46cb6aa83fbd64a64a8a87d782476438abb658ce89b89c...</td>\n",
       "      <td>-0.042661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>273</td>\n",
       "      <td>26e5d74e138cf23f5d65aab46fdc59f6421e97ccf1ab8e...</td>\n",
       "      <td>-0.014090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>273</td>\n",
       "      <td>9dd2e69f186ef4eb076c646a8b182e936af2667793d143...</td>\n",
       "      <td>-0.003048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4129</th>\n",
       "      <td>273</td>\n",
       "      <td>002647639e3b83fd884eed0eddf72a702f15c5d70fb75d...</td>\n",
       "      <td>-0.002502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>273</td>\n",
       "      <td>7d840103d2370a80cc9b8376bfaf04b2aa5ff46bcbab03...</td>\n",
       "      <td>0.019638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>273</td>\n",
       "      <td>8cb714eb33a36b24b868155daac9efc9f9ff5c5e01bee1...</td>\n",
       "      <td>-0.025965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20510 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date                                                 id     value\n",
       "0      269  c6e83eda40042dab1af117e195d542f00a417627e3173a... -0.036208\n",
       "1      269  97ae3194605438cbd2c59a3827f7c615dafa40d6cc3f42...  0.015091\n",
       "2      269  310382927ec56f64c6f2f834fd320c9f732e26df639e67... -0.004857\n",
       "3      269  6e963f295f4ec1dc921be47638dba304f486ab2efd313c... -0.031028\n",
       "4      269  46cb6aa83fbd64a64a8a87d782476438abb658ce89b89c... -0.042661\n",
       "...    ...                                                ...       ...\n",
       "4127   273  26e5d74e138cf23f5d65aab46fdc59f6421e97ccf1ab8e... -0.014090\n",
       "4128   273  9dd2e69f186ef4eb076c646a8b182e936af2667793d143... -0.003048\n",
       "4129   273  002647639e3b83fd884eed0eddf72a702f15c5d70fb75d... -0.002502\n",
       "4130   273  7d840103d2370a80cc9b8376bfaf04b2aa5ff46bcbab03...  0.019638\n",
       "4131   273  8cb714eb33a36b24b868155daac9efc9f9ff5c5e01bee1... -0.025965\n",
       "\n",
       "[20510 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crunch.test(force_first_train=True, train_frequency=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crunch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
