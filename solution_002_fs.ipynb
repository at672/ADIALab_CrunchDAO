{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADIA LAB CrunchDAO Notebook Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up workspace cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 0\n",
    "!pip3 install crunch-cli --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "## STEP 2: Run this cell to load the data\n",
    "import crunch\n",
    "crunch = crunch.load_notebook(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stale-avi: already exists (use --force to override)\n",
      "c:\\GitHub\\CrunchDAO\\stale-avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "## STEP 3\n",
    "!crunch --notebook setup stale-avi --token 4tT3eWdXumNoCG7b7FKZJLN17fYnww6gSl6yeHy4PdsYPeqTnrKiq49hRb4sdmm2\n",
    "%cd stale-avi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPORTS I MAY USE\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # This is not advised in general, but it is used in this notebook to clean the presentation of results\n",
    "\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from hyperopt import hp, fmin, tpe, Trials\n",
    "#from hyperopt.pyll.base import scope\n",
    "#from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import dump, load\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "#import datatable as dtable\n",
    "import typing\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BORUTA feature selection\n",
    "np.random.seed(42)\n",
    "\n",
    "def get_X_boruta(X_train):\n",
    "    X_shadow = X_train.apply(np.random.permutation)\n",
    "    X_shadow.columns = ['shadow_' + feat for feat in X_train.columns]\n",
    "    X_boruta = pd.concat([X_train, X_shadow], axis=1)\n",
    "    return X_boruta\n",
    "\n",
    "def get_random_bar(X_boruta, y, X_train, model):\n",
    "    model.fit(X_boruta, np.array(y).ravel())\n",
    "    feat_imp_X = model.feature_importances_[:len(X_train.columns)]\n",
    "    feat_imp_shadow = model.feature_importances_[len(X_train.columns):]\n",
    "    hits = feat_imp_X > feat_imp_shadow.max()\n",
    "    return feat_imp_X, feat_imp_shadow, hits\n",
    "\n",
    "def get_relevant_features(X_train, hits):\n",
    "    features = X_train.columns.values\n",
    "    relevant_features = []\n",
    "    for index, value in enumerate(hits):\n",
    "        if value == True:\n",
    "            relevant_features.append(features[index])\n",
    "    return relevant_features\n",
    "\n",
    "def store_list_as_json(data_list, folder_path, file_name):\n",
    "    json_data = json.dumps(data_list)\n",
    "    file_path = folder_path + \"/\" + file_name\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "        \n",
    "        \n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        json_data = json_file.read()\n",
    "    data_list = json.loads(json_data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "\n",
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "          model_directory_path: str = \"resources\") -> None:\n",
    "    \"\"\"\n",
    "    Do your model training here.\n",
    "    At each retrain this function will have to save an updated version of\n",
    "    the model under the model_directiory_path, as in the example below.\n",
    "    Note: You can use other serialization methods than joblib.dump(), as\n",
    "    long as it matches what reads the model in infer().\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: the data to train the model.\n",
    "        model_directory_path: the path to save your updated model\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #flush the memory\n",
    "    ## For model construction\n",
    "    #     alpha = 5\n",
    "    # Num_hidden_limit = X_train.shape[0] / ( alpha * (X_train.shape[1] + 1) )\n",
    "    # #alpha = 2 --> 800\n",
    "    # #alpha = 5 --> 320\n",
    "    # #alpha = 10 --> 160\n",
    "\n",
    "    # Cutting the dataset to the last n_cut dates.\n",
    "    print(\"Preprocessing: Boruta feature selection\")\n",
    "    n_cut = 5\n",
    "    \n",
    "    cut_df_X = X_train[X_train['date'] >= int(len(X_train['date'].unique()) - n_cut)]\n",
    "    cut_df_y = y_train[y_train['date'] >= int(len(y_train['date'].unique()) - n_cut)]\n",
    "    \n",
    "    X_train = cut_df_X.drop(columns=['id', 'date'])\n",
    "    y_train = cut_df_y.drop(columns=['id', 'date'])\n",
    "    \n",
    "    \n",
    "    # Feature selection step\n",
    "    model = RandomForestRegressor(n_estimators=30, max_depth=3, n_jobs=30, random_state=42)\n",
    "    \n",
    "    n_iter = 100\n",
    "    n_relevant_features = [len(X_train.columns)]\n",
    "\n",
    "    rel_Xtrain = X_train.copy()\n",
    "    print(\"Iterating for Boruta selection\")\n",
    "    for i in range(1,n_iter+1):\n",
    "        X_boruta = get_X_boruta(rel_Xtrain)\n",
    "        feat_importance_X, feat_importance_shadow, hits = get_random_bar(X_boruta, y_train, rel_Xtrain, model)\n",
    "        relevant_features = get_relevant_features(rel_Xtrain, hits)\n",
    "        n_relevant_features.append(len(relevant_features))\n",
    "        rel_Xtrain = rel_Xtrain.loc[:,relevant_features]\n",
    "        if n_relevant_features[i] == n_relevant_features[i-1]:\n",
    "            break\n",
    "    print(\"Iterated through \", i, \" times for Boruta selection.\")\n",
    "    relevant_features = rel_Xtrain.columns.to_list()\n",
    "    n_features = rel_Xtrain.shape[1]\n",
    "    relevant_features.insert(0, 'date')\n",
    "    relevant_features.insert(0,'id')\n",
    "    \n",
    "    # Saving the selected features in 'resources' file\n",
    "    print(\"saving the selected features\")\n",
    "    folder_path = 'resources'\n",
    "    file_name = 'selected_feat.json'\n",
    "    store_list_as_json(relevant_features, folder_path, file_name)\n",
    "\n",
    "    # # Training the model\n",
    "    # print(\"training...\")\n",
    "    # model.fit(rel_Xtrain, np.array(y_train).ravel())\n",
    "    # print(model)\n",
    "    \n",
    "    # # Saving the model in 'resources' file\n",
    "    # model_directory_path = 'resources'\n",
    "    # model_pathname = Path(model_directory_path) / \"model.joblib\"\n",
    "    # print(f\"Saving model in {model_pathname}\")\n",
    "    # joblib.dump(model, model_pathname)\n",
    "\n",
    "\n",
    "    tf.random.set_seed(6741)\n",
    "\n",
    "    train_samples = X_train.shape[0]\n",
    "    #n_features = X_train.shape[1] - 2\n",
    "    #can this fit in memory?\n",
    "    batch_size = 74267\n",
    "    #needs to be small enough that it can fit in memory. 2500 x 461 should be good\n",
    "    batch_size = 2500\n",
    "    #batch_size = 1000\n",
    "\n",
    "    alpha = 8\n",
    "    num_hidden_layer_1 = int( train_samples / ( alpha * (n_features + 1) ) )\n",
    "\n",
    "    ## do not include batch size when constructing input layer.\n",
    "    model = tf.keras.models.Sequential([\n",
    "        #tf.keras.layers.Flatten(input_shape = X_train.shape),\n",
    "        tf.keras.layers.Input(shape = (None, train_samples, n_features), batch_size = batch_size, name = \"Input_Layer\"),\n",
    "        tf.keras.layers.Dense(num_hidden_layer_1, activation='swish', activity_regularizer = tf.keras.regularizers.L2(0.03)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(600, activation='tanh', activity_regularizer = tf.keras.regularizers.L2(0.03)),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(400, activation='swish', activity_regularizer = tf.keras.regularizers.L2(0.03)),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(200, activation='tanh', activity_regularizer = tf.keras.regularizers.L2(0.03)),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(10, activation ='tanh'),\n",
    "        tf.keras.layers.Dense(1, activation = None)\n",
    "    ])\n",
    "\n",
    "    #monitor = val_loss\n",
    "    #es = EarlyStopping(monitor = 'val_action_AUC', min_delta = 1e-4, patience = 10, mode = 'max', \n",
    "    #                       baseline = None, restore_best_weights = True, verbose = 0)\n",
    "\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience = 2, restore_best_weights = True)\n",
    "    \n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=2, min_lr=0.001)\n",
    "    #my_callbacks = [ reduce_lr, early_stopping ]\n",
    "    my_callbacks = [ early_stopping ]\n",
    "    #if using KL Divergence as loss, don't add it as a metric\n",
    "    model.compile(\n",
    "        optimizer= tf.keras.optimizers.Adam(learning_rate = 0.001) ,\n",
    "        #optimizer = tf.keras.optimizers.experimental.SGD( learning_rate = 0.001),\n",
    "        #loss = tf.keras.losses.BinaryCrossentropy(from_logits = True) ,\n",
    "        #loss = tf.keras.losses.KLDivergence(),\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "        )\n",
    "\n",
    "    # #Can do this if wanted\n",
    "    # model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=1e-3) ,\n",
    "    #     #loss = tf.keras.losses.BinaryCrossentropy(from_logits = True) ,\n",
    "    #     loss = 'mse',\n",
    "    #     metrics = [tf.keras.metrics.KLDivergence()]\n",
    "    #     )\n",
    "\n",
    "    # training the model\n",
    "    print(\"training...\")\n",
    "    history = model.fit(\n",
    "        x = rel_Xtrain,\n",
    "        y = np.array(y_train).ravel(),\n",
    "        epochs = 100,\n",
    "        steps_per_epoch = int( np.ceil(train_samples / batch_size)),\n",
    "        validation_split = 0.3,\n",
    "        shuffle = True,\n",
    "        workers = 8,\n",
    "        use_multiprocessing = True,\n",
    "        verbose = 1,\n",
    "        callbacks = my_callbacks\n",
    "     )\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "    ## SAVE THE MODEL\n",
    "    # make sure that the train function correctly save the trained model\n",
    "    # in the model_directory_path\n",
    "    model_pathname = Path(model_directory_path) / \"model.joblib\"\n",
    "    print(f\"Saving model in {model_pathname}\")\n",
    "    joblib.dump(model, model_pathname)\n",
    "\n",
    "    #Save history\n",
    "    # convert the history.history dict to a pandas DataFrame:     \n",
    "    # TODO: Figure out how to catch a PermissionError so this doesn't fail on the cloud.\n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    hist_csv_file = 'resources/epoch_history.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "\n",
    "def infer(X_test: pd.DataFrame,\n",
    "          model_directory_path: str = \"resources\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Do your inference here.\n",
    "    This function will load the model saved at the previous iteration and use\n",
    "    it to produce your inference on the current date.\n",
    "    It is mandatory to send your inferences with the ids so the system\n",
    "    can match it correctly.\n",
    "    \n",
    "    Args:\n",
    "        model_directory_path: the path to the directory to the directory in wich we will be saving your updated model.\n",
    "        X_test: the independant  variables of the current date passed to your model.\n",
    "\n",
    "    Returns:\n",
    "        A dataframe (date, id, value) with the inferences of your model for the current date.\n",
    "    \"\"\"\n",
    "    # Loading the selected features\n",
    "    file_path = \"resources/selected_feat.json\"\n",
    "    selected_features = read_json_file(file_path)\n",
    "\n",
    "    # loading the model saved by the train function at previous iteration\n",
    "    model = joblib.load(Path(model_directory_path) / \"model.joblib\")\n",
    "    \n",
    "    # Creating the predicted label dataframe with correct dates and ids\n",
    "    X_test = X_test[selected_features]\n",
    "    y_test_predicted = X_test[[\"date\", \"id\"]].copy()\n",
    "    y_test_predicted[\"value\"] = model.predict(X_test.iloc[:, 2:])\n",
    "\n",
    "    return y_test_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Locally - Part 1: Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\\X_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_train.parquet\n",
      "already exists: file length match\n",
      "download data\\y_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/y_train.parquet\n",
      "already exists: file length match\n",
      "download data\\X_test.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_test_reduced.parquet\n",
      "already exists: file length match\n",
      "download data\\y_test.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/y_test_reduced.parquet\n",
      "already exists: file length match\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# Getting the data\n",
    "X_train, y_train, X_test = crunch.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting (X_train, y_train) in X_train_local, X_test_local, y_train_local, y_test_local\n",
      "X_train_local shape  (542200, 463)\n",
      "X_test_local shape  (200470, 463)\n",
      "y_train_local shape  (542200, 3)\n",
      "y_test_local shape  (200470, 3)\n"
     ]
    }
   ],
   "source": [
    "def temporal_train_test_split(X_train_loc, y_train_loc, test_size=0.2):\n",
    "    unique_dates = X_train_loc.date.unique()\n",
    "    split_date = unique_dates[int(len(unique_dates)*(1-test_size))]\n",
    "    X_train_local = X_train_loc[X_train_loc['date'] <= split_date]\n",
    "    X_test_local = X_train_loc[X_train_loc['date'] > split_date]\n",
    "    \n",
    "    y_train_local = y_train_loc[y_train_loc['date'] <= split_date]\n",
    "    y_test_local = y_train_loc[y_train_loc['date'] > split_date]\n",
    "    \n",
    "    return X_train_local, X_test_local, y_train_local, y_test_local\n",
    "\n",
    "print(\"Splitting (X_train, y_train) in X_train_local, X_test_local, y_train_local, y_test_local\")\n",
    "X_train_local, X_test_local, y_train_local, y_test_local = temporal_train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.20\n",
    ")\n",
    "\n",
    "print(\"X_train_local shape \", X_train_local.shape)\n",
    "print(\"X_test_local shape \", X_test_local.shape)\n",
    "print(\"y_train_local shape \", y_train_local.shape)\n",
    "print(\"y_test_local shape \", y_test_local.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: Boruta feature selection\n",
      "Iterating for Boruta selection\n",
      "Iterated through  5  times for Boruta selection.\n",
      "saving the selected features\n",
      "training...\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 2s 103ms/step - loss: 1.6111 - val_loss: 0.9342\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 1.2396 - val_loss: 0.7660\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 1.0335 - val_loss: 0.6614\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.8898 - val_loss: 0.5954\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 76ms/step - loss: 0.7849 - val_loss: 0.5489\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.7092 - val_loss: 0.5156\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.6533 - val_loss: 0.4910\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.6087 - val_loss: 0.4723\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.5732 - val_loss: 0.4583\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.5456 - val_loss: 0.4459\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.5229 - val_loss: 0.4353\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.5032 - val_loss: 0.4272\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.4876 - val_loss: 0.4199\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.4734 - val_loss: 0.4135\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.4606 - val_loss: 0.4079\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 1s 77ms/step - loss: 0.4506 - val_loss: 0.4030\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.4416 - val_loss: 0.3981\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 78ms/step - loss: 0.4326 - val_loss: 0.3939\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.4256 - val_loss: 0.3900\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.4187 - val_loss: 0.3871\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.4125 - val_loss: 0.3842\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.4078 - val_loss: 0.3813\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.4031 - val_loss: 0.3785\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3986 - val_loss: 0.3765\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3940 - val_loss: 0.3740\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3902 - val_loss: 0.3720\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3868 - val_loss: 0.3696\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3845 - val_loss: 0.3678\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3808 - val_loss: 0.3662\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.3784 - val_loss: 0.3646\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.3760 - val_loss: 0.3634\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3732 - val_loss: 0.3620\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3714 - val_loss: 0.3603\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3689 - val_loss: 0.3591\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3675 - val_loss: 0.3578\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.3653 - val_loss: 0.3564\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3638 - val_loss: 0.3552\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3620 - val_loss: 0.3544\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3602 - val_loss: 0.3531\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.3592 - val_loss: 0.3522\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.3576 - val_loss: 0.3514\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3564 - val_loss: 0.3508\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.3552 - val_loss: 0.3499\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3539 - val_loss: 0.3486\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.3527 - val_loss: 0.3480\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3519 - val_loss: 0.3471\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.3514 - val_loss: 0.3467\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3501 - val_loss: 0.3461\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.3493 - val_loss: 0.3455\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.3481 - val_loss: 0.3449\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.3476 - val_loss: 0.3442\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 76ms/step - loss: 0.3467 - val_loss: 0.3436\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.3456 - val_loss: 0.3429\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3455 - val_loss: 0.3425\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3449 - val_loss: 0.3419\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.3441 - val_loss: 0.3418\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.3432 - val_loss: 0.3411\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.3426 - val_loss: 0.3408\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3422 - val_loss: 0.3403\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3411 - val_loss: 0.3399\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3409 - val_loss: 0.3392\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3404 - val_loss: 0.3389\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3399 - val_loss: 0.3384\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3392 - val_loss: 0.3379\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3390 - val_loss: 0.3376\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3388 - val_loss: 0.3377\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3381 - val_loss: 0.3376\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3382 - val_loss: 0.3374\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3378 - val_loss: 0.3363\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3374 - val_loss: 0.3365\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3369 - val_loss: 0.3364\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3365 - val_loss: 0.3358\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3366 - val_loss: 0.3357\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3360 - val_loss: 0.3353\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3358 - val_loss: 0.3351\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3352 - val_loss: 0.3347\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3354 - val_loss: 0.3346\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3349 - val_loss: 0.3345\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3349 - val_loss: 0.3344\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3346 - val_loss: 0.3346\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 1s 77ms/step - loss: 0.3348 - val_loss: 0.3341\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.3345 - val_loss: 0.3339\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3342 - val_loss: 0.3340\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3338 - val_loss: 0.3337\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3337 - val_loss: 0.3333\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3337 - val_loss: 0.3338\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.3337 - val_loss: 0.3334\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3333 - val_loss: 0.3332\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3330 - val_loss: 0.3326\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.3328 - val_loss: 0.3326\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 78ms/step - loss: 0.3329 - val_loss: 0.3330\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.3328 - val_loss: 0.3338\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3328 - val_loss: 0.3326\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3327 - val_loss: 0.3328\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 77ms/step - loss: 0.3326 - val_loss: 0.3326\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.3321 - val_loss: 0.3319\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 76ms/step - loss: 0.3324 - val_loss: 0.3324\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.3322 - val_loss: 0.3327\n",
      "Training complete.\n",
      "Saving model in resources\\model.joblib\n",
      "#############\n",
      "Inference\n",
      "6265/6265 [==============================] - 9s 1ms/step\n",
      "Spearman's correlation 1.6102931712427195\n"
     ]
    }
   ],
   "source": [
    "#flush memory\n",
    "tf.keras.backend.clear_session()\n",
    "# Training. It may require a few minutes.\n",
    "train(X_train_local, y_train_local)\n",
    "\n",
    "print(\"#############\")\n",
    "print(\"Inference\")\n",
    "y_test_local_pred = infer(X_test_local, model_directory_path=\"resources\")\n",
    "score = spearmanr(y_test_local[\"y\"], y_test_local_pred[\"value\"])[0] * 100\n",
    "print(f\"Spearman's correlation {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model Locally Part 2: Actual Test Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove unused data to release memory\n"
     ]
    }
   ],
   "source": [
    "print(\"Remove unused data to release memory\")\n",
    "del X_train, y_train, X_test, X_train_local, X_test_local, y_train_local, y_test_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignoring cell #19: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 55)\n",
      "ignoring cell #23: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 55)\n",
      "ignoring cell #45: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m09:48:18\u001b[0m \u001b[31mforbidden library: requests\u001b[0m\n",
      "\u001b[32m09:48:18\u001b[0m \u001b[33m\u001b[0m\n",
      "\u001b[32m09:48:18\u001b[0m running local test\n",
      "\u001b[32m09:48:18\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
      "\u001b[32m09:48:18\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\\X_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_train.parquet\n",
      "already exists: file length match\n",
      "download data\\y_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/y_train.parquet\n",
      "already exists: file length match\n",
      "download data\\X_test.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_test_reduced.parquet\n",
      "already exists: file length match\n",
      "download data\\y_test.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/y_test_reduced.parquet\n",
      "already exists: file length match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m09:48:21\u001b[0m \u001b[33m---\u001b[0m\n",
      "\u001b[32m09:48:21\u001b[0m \u001b[33mloop: moon=269 train=True (1/5)\u001b[0m\n",
      "\u001b[32m09:48:21\u001b[0m \u001b[33mcall: train\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Epoch 1/20\n",
      "296/296 [==============================] - 25s 81ms/step - loss: 0.9868 - val_loss: 0.3759\n",
      "Epoch 2/20\n",
      "296/296 [==============================] - 25s 84ms/step - loss: 0.3579 - val_loss: 0.3431\n",
      "Epoch 3/20\n",
      "296/296 [==============================] - 25s 84ms/step - loss: 0.3415 - val_loss: 0.3382\n",
      "Epoch 4/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3383 - val_loss: 0.3367\n",
      "Epoch 5/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3374 - val_loss: 0.3365\n",
      "Epoch 6/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3371 - val_loss: 0.3366\n",
      "Epoch 7/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3372 - val_loss: 0.3361\n",
      "Epoch 8/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3372 - val_loss: 0.3367\n",
      "Epoch 9/20\n",
      "296/296 [==============================] - 25s 83ms/step - loss: 0.3369 - val_loss: 0.3370\n",
      "Epoch 10/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3369 - val_loss: 0.3368\n",
      "Epoch 11/20\n",
      "296/296 [==============================] - 24s 82ms/step - loss: 0.3364 - val_loss: 0.3368\n",
      "Epoch 12/20\n",
      "296/296 [==============================] - 24s 82ms/step - loss: 0.3365 - val_loss: 0.3361\n",
      "Epoch 13/20\n",
      "296/296 [==============================] - 24s 81ms/step - loss: 0.3361 - val_loss: 0.3359\n",
      "Epoch 14/20\n",
      "296/296 [==============================] - 23s 79ms/step - loss: 0.3360 - val_loss: 0.3366\n",
      "Epoch 15/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3361 - val_loss: 0.3366\n",
      "Epoch 16/20\n",
      "296/296 [==============================] - 23s 79ms/step - loss: 0.3359 - val_loss: 0.3369\n",
      "Epoch 17/20\n",
      "296/296 [==============================] - 24s 81ms/step - loss: 0.3359 - val_loss: 0.3366\n",
      "Epoch 18/20\n",
      "296/296 [==============================] - 24s 81ms/step - loss: 0.3358 - val_loss: 0.3367\n",
      "Epoch 19/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3358 - val_loss: 0.3363\n",
      "Epoch 20/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.3358 - val_loss: 0.3360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m09:56:22\u001b[0m \u001b[33mcall: infer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Saving model in resources\\model.joblib\n",
      "125/125 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m09:56:23\u001b[0m \u001b[33m---\u001b[0m\n",
      "\u001b[32m09:56:23\u001b[0m \u001b[33mloop: moon=270 train=True (2/5)\u001b[0m\n",
      "\u001b[32m09:56:23\u001b[0m \u001b[33mcall: train\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Epoch 1/20\n",
      "298/298 [==============================] - 28s 85ms/step - loss: 0.9873 - val_loss: 0.3756\n",
      "Epoch 2/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3585 - val_loss: 0.3442\n",
      "Epoch 3/20\n",
      "298/298 [==============================] - 25s 85ms/step - loss: 0.3423 - val_loss: 0.3391\n",
      "Epoch 4/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3387 - val_loss: 0.3372\n",
      "Epoch 5/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3375 - val_loss: 0.3367\n",
      "Epoch 6/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3371 - val_loss: 0.3366\n",
      "Epoch 7/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3372 - val_loss: 0.3365\n",
      "Epoch 8/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3374 - val_loss: 0.3369\n",
      "Epoch 9/20\n",
      "298/298 [==============================] - 26s 86ms/step - loss: 0.3372 - val_loss: 0.3370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:00:17\u001b[0m \u001b[33mcall: infer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Saving model in resources\\model.joblib\n",
      "131/131 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:00:18\u001b[0m \u001b[33m---\u001b[0m\n",
      "\u001b[32m10:00:18\u001b[0m \u001b[33mloop: moon=271 train=False (3/5)\u001b[0m\n",
      "\u001b[32m10:00:18\u001b[0m \u001b[33mcall: infer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:00:18\u001b[0m \u001b[33m---\u001b[0m\n",
      "\u001b[32m10:00:18\u001b[0m \u001b[33mloop: moon=272 train=True (4/5)\u001b[0m\n",
      "\u001b[32m10:00:18\u001b[0m \u001b[33mcall: train\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Epoch 1/20\n",
      "301/301 [==============================] - 26s 83ms/step - loss: 0.9865 - val_loss: 0.3753\n",
      "Epoch 2/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3582 - val_loss: 0.3433\n",
      "Epoch 3/20\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.3419 - val_loss: 0.3385\n",
      "Epoch 4/20\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.3387 - val_loss: 0.3372\n",
      "Epoch 5/20\n",
      "301/301 [==============================] - 25s 84ms/step - loss: 0.3379 - val_loss: 0.3366\n",
      "Epoch 6/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3378 - val_loss: 0.3366\n",
      "Epoch 7/20\n",
      "301/301 [==============================] - 25s 84ms/step - loss: 0.3376 - val_loss: 0.3365\n",
      "Epoch 8/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3372 - val_loss: 0.3375\n",
      "Epoch 9/20\n",
      "301/301 [==============================] - 26s 85ms/step - loss: 0.3369 - val_loss: 0.3361\n",
      "Epoch 10/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3367 - val_loss: 0.3359\n",
      "Epoch 11/20\n",
      "301/301 [==============================] - 25s 85ms/step - loss: 0.3362 - val_loss: 0.3356\n",
      "Epoch 12/20\n",
      "301/301 [==============================] - 25s 84ms/step - loss: 0.3360 - val_loss: 0.3361\n",
      "Epoch 13/20\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.3359 - val_loss: 0.3359\n",
      "Epoch 14/20\n",
      "301/301 [==============================] - 24s 81ms/step - loss: 0.3358 - val_loss: 0.3370\n",
      "Epoch 15/20\n",
      "301/301 [==============================] - 25s 84ms/step - loss: 0.3362 - val_loss: 0.3353\n",
      "Epoch 16/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3356 - val_loss: 0.3360\n",
      "Epoch 17/20\n",
      "301/301 [==============================] - 25s 82ms/step - loss: 0.3355 - val_loss: 0.3360\n",
      "Epoch 18/20\n",
      "301/301 [==============================] - 24s 81ms/step - loss: 0.3358 - val_loss: 0.3358\n",
      "Epoch 19/20\n",
      "301/301 [==============================] - 26s 85ms/step - loss: 0.3354 - val_loss: 0.3359\n",
      "Epoch 20/20\n",
      "301/301 [==============================] - 25s 83ms/step - loss: 0.3355 - val_loss: 0.3358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:08:41\u001b[0m \u001b[33mcall: infer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Saving model in resources\\model.joblib\n",
      "128/128 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:08:42\u001b[0m \u001b[33m---\u001b[0m\n",
      "\u001b[32m10:08:42\u001b[0m \u001b[33mloop: moon=273 train=False (5/5)\u001b[0m\n",
      "\u001b[32m10:08:42\u001b[0m \u001b[33mcall: infer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:08:43\u001b[0m \u001b[33mprediction_path=data\\prediction.csv\u001b[0m\n",
      "\u001b[32m10:08:43\u001b[0m \u001b[33mduration: time=00:20:24\u001b[0m\n",
      "\u001b[32m10:08:43\u001b[0m \u001b[33mmemory: before=\"5.77 GB\" after=\"8.70 GB\" consumed=\"2.93 GB\"\u001b[0m\n",
      "\u001b[32m10:08:43\u001b[0m \u001b[33mlocal test succesfully run!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>269</td>\n",
       "      <td>c6e83eda40042dab1af117e195d542f00a417627e3173a...</td>\n",
       "      <td>-0.036208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>269</td>\n",
       "      <td>97ae3194605438cbd2c59a3827f7c615dafa40d6cc3f42...</td>\n",
       "      <td>0.015091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>269</td>\n",
       "      <td>310382927ec56f64c6f2f834fd320c9f732e26df639e67...</td>\n",
       "      <td>-0.004857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>269</td>\n",
       "      <td>6e963f295f4ec1dc921be47638dba304f486ab2efd313c...</td>\n",
       "      <td>-0.031028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>269</td>\n",
       "      <td>46cb6aa83fbd64a64a8a87d782476438abb658ce89b89c...</td>\n",
       "      <td>-0.042661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>273</td>\n",
       "      <td>26e5d74e138cf23f5d65aab46fdc59f6421e97ccf1ab8e...</td>\n",
       "      <td>-0.014090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>273</td>\n",
       "      <td>9dd2e69f186ef4eb076c646a8b182e936af2667793d143...</td>\n",
       "      <td>-0.003048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4129</th>\n",
       "      <td>273</td>\n",
       "      <td>002647639e3b83fd884eed0eddf72a702f15c5d70fb75d...</td>\n",
       "      <td>-0.002502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>273</td>\n",
       "      <td>7d840103d2370a80cc9b8376bfaf04b2aa5ff46bcbab03...</td>\n",
       "      <td>0.019638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>273</td>\n",
       "      <td>8cb714eb33a36b24b868155daac9efc9f9ff5c5e01bee1...</td>\n",
       "      <td>-0.025965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20510 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date                                                 id     value\n",
       "0      269  c6e83eda40042dab1af117e195d542f00a417627e3173a... -0.036208\n",
       "1      269  97ae3194605438cbd2c59a3827f7c615dafa40d6cc3f42...  0.015091\n",
       "2      269  310382927ec56f64c6f2f834fd320c9f732e26df639e67... -0.004857\n",
       "3      269  6e963f295f4ec1dc921be47638dba304f486ab2efd313c... -0.031028\n",
       "4      269  46cb6aa83fbd64a64a8a87d782476438abb658ce89b89c... -0.042661\n",
       "...    ...                                                ...       ...\n",
       "4127   273  26e5d74e138cf23f5d65aab46fdc59f6421e97ccf1ab8e... -0.014090\n",
       "4128   273  9dd2e69f186ef4eb076c646a8b182e936af2667793d143... -0.003048\n",
       "4129   273  002647639e3b83fd884eed0eddf72a702f15c5d70fb75d... -0.002502\n",
       "4130   273  7d840103d2370a80cc9b8376bfaf04b2aa5ff46bcbab03...  0.019638\n",
       "4131   273  8cb714eb33a36b24b868155daac9efc9f9ff5c5e01bee1... -0.025965\n",
       "\n",
       "[20510 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crunch.test(force_first_train=True, train_frequency=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crunch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
